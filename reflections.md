Reflections
===========

<!--
This file generated by the build script at ./Build.hs from the files in
./reflections.  If you want to edit this, edit those instead!
-->

Table of Contents
-----------------

* [Day 1](#day-1)
* [Day 2](#day-2)
* [Day 3](#day-3) *(no reflection yet)*

Day 1
------

<!--
This section is generated and compiled by the build script at ./Build.hs from
the file `./reflections/day01.md`.  If you want to edit this, edit
that file instead!
-->

*[Prompt][d01p]* / *[Code][d01g]*

[d01p]: https://adventofcode.com/2022/day/1
[d01g]: https://github.com/egnwd/advent/blob/2022/src/AOC/Challenge/Day01.hs

My prewritten parser almost worked... just needed to install the split library...

But in any case parsing was relatively straight forward.
We simply need to split into the elf groups and the split the individual lines and read the numbers.

```haskell
parse :: String -> [[Int]]
parse = map (map read) . map lines . splitOn "\n\n"
```

To find the top one for part a I originally went for the following `foldMap` approach using the useful [`Max` Semigroup](https://hackage.haskell.org/package/base-4.17.0.0/docs/Data-Semigroup.html#t:Max)

```haskell
solve :: [[Int]] -> Int
solve = getMax . foldMap (Max . sum)
```

Which, yay!, works but when we get to part 2 we can make more generic and reuse the part 2 implementation.

```haskell
solve :: [[Int]] -> Int
solve = sum . take 3 . reverse . sort . map sum
```

This solution does away with `Max` in favour of just sorting the summed list.

We can then pass in how many elves are contributing snacks:

```haskell
solve :: Int -> [[Int]] -> Int
solve n = sum . take n . reverse . sort . map sum
```

and tada! We've started our long journey[^1] into the jungle!

[^1]: _journey_ would be a great codename for a project


### Day 1 Benchmarks

```
>> Day 01a
benchmarking...
time                 24.49 μs   (24.43 μs .. 24.58 μs)
                     1.000 R²   (1.000 R² .. 1.000 R²)
mean                 24.53 μs   (24.50 μs .. 24.59 μs)
std dev              158.0 ns   (127.0 ns .. 192.4 ns)

* parsing and formatting times excluded

>> Day 01b
benchmarking...
time                 26.14 μs   (25.61 μs .. 27.30 μs)
                     0.993 R²   (0.982 R² .. 1.000 R²)
mean                 25.94 μs   (25.72 μs .. 27.12 μs)
std dev              1.304 μs   (239.6 ns .. 3.152 μs)
variance introduced by outliers: 58% (severely inflated)

* parsing and formatting times excluded
```



Day 2
------

<!--
This section is generated and compiled by the build script at ./Build.hs from
the file `./reflections/day02.md`.  If you want to edit this, edit
that file instead!
-->

*[Prompt][d02p]* / *[Code][d02g]*

[d02p]: https://adventofcode.com/2022/day/2
[d02g]: https://github.com/egnwd/advent/blob/2022/src/AOC/Challenge/Day02.hs

Hmm... Well I messed up.

I had the solution fairly quickly by hard coding the mappings suggested and lots of lookups...
and then decided the intended solution was to find the *maximum* score given any strategy...

which does have a nice little form

```haskell
strategies = map (zip "ABC") (permutations "XYZ")
```

and then you simply need to find the maximum from applying all the strategies.


But that's wrong, so let's simplify.

First we can set up a parser, using the default lookup:

```haskell
parseGame = do
    opp <- Rock <$ "A" <|> Paper <$ "B" <|> Scissors <$ "C"
    pSpace
    you <- Rock <$ "X" <|> Paper <$ "Y" <|> Scissors <$ "Z"
    return (opp, you)
```

and then we can score a game given some rules on the winner:

```haskell
Paper    `play` Rock     = Win
Rock     `play` Scissors = Win
Scissors `play` Paper    = Win
a        `play` b
  | a == b = Draw
  | otherwise = Lose
```

I wasn't hugely happy with this, I tried an implementation with `Ord` and another with `Map`
but this was still the simplest, and it's a total function as we've parsed the inputs into nice types.

Then given the scoring functions set out in the puzzle:

```haskell
score you out = scoreRPS you + scoreOutcome out

scoreRPS = \case
    Rock     -> 1
    Paper    -> 2
    Scissors -> 3

scoreOutcome = \case
    Win  -> 6
    Draw -> 3
    Lose -> 0
```

we can score a game simply using ``\opp you -> score you (you `play` opp)``.

Finally.

Then for part two, much the same we just need a different lookup and different parser.

```haskell
parseStrategy = do
    opp <- Rock <$ "A" <|> Paper <$ "B" <|> Scissors <$ "C"
    pSpace
    out <- Lose <$ "X" <|> Draw <$ "Y" <|> Win <$ "Z"
    return (opp, out)

whatToPlay a        Draw = a
whatToPlay Paper    Win  = Scissors
whatToPlay Rock     Win  = Paper
whatToPlay Scissors Win  = Rock
whatToPlay Paper    Lose = Rock
whatToPlay Rock     Lose = Scissors
whatToPlay Scissors Lose = Paper
```

again, I'm not a fan of huge lookups but there we go.

putting together what we already have gives us ``\opp out -> score (whatToPlay opp out) out``.

Two stars, but not the most satisfying.


### Day 2 Benchmarks

```
>> Day 02a
benchmarking...
time                 5.104 ms   (5.050 ms .. 5.174 ms)
                     0.999 R²   (0.998 R² .. 0.999 R²)
mean                 5.089 ms   (5.033 ms .. 5.130 ms)
std dev              142.0 μs   (113.0 μs .. 190.2 μs)
variance introduced by outliers: 11% (moderately inflated)

>> Day 02b
benchmarking...
time                 2.760 ms   (2.734 ms .. 2.813 ms)
                     0.987 R²   (0.962 R² .. 0.999 R²)
mean                 2.812 ms   (2.771 ms .. 3.004 ms)
std dev              237.1 μs   (88.73 μs .. 499.2 μs)
variance introduced by outliers: 57% (severely inflated)
```



Day 3
------

<!--
This section is generated and compiled by the build script at ./Build.hs from
the file `./reflections/day03.md`.  If you want to edit this, edit
that file instead!
-->

*[Prompt][d03p]* / *[Code][d03g]*

[d03p]: https://adventofcode.com/2022/day/3
[d03g]: https://github.com/egnwd/advent/blob/2022/src/AOC/Challenge/Day03.hs

*Reflection not yet written -- please check back later!*

### Day 3 Benchmarks

```
>> Day 03a
benchmarking...
time                 255.3 μs   (254.4 μs .. 256.1 μs)
                     1.000 R²   (1.000 R² .. 1.000 R²)
mean                 254.9 μs   (254.5 μs .. 255.4 μs)
std dev              1.572 μs   (1.232 μs .. 2.181 μs)

* parsing and formatting times excluded

>> Day 03b
benchmarking...
time                 292.2 μs   (291.6 μs .. 292.9 μs)
                     1.000 R²   (1.000 R² .. 1.000 R²)
mean                 291.9 μs   (291.4 μs .. 292.6 μs)
std dev              2.047 μs   (1.555 μs .. 2.879 μs)

* parsing and formatting times excluded
```

